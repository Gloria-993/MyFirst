# 我对关联数据的看法

2014.12.21   作者：鲍捷

***

*（有人问我对关联数据的看法，这是我的回信）*

## 第一部分

关联数据是指Linked Data吧？这个话题很大，很难一次说清楚。我的博客上有些零散的文章。总的来说，我认为RDF为代表的路线在实践中是行不通的。URI为基础的命名的标准化，现阶段是超越产业发展规律的东西，商业上没有需求。语义网所要求的推理，在实际部署中也是行不通的，对机器的计算复杂性，对数据质量的要求，对程序员的认知复杂性要求都太高了。因为有这些脱离实际的设想，无论是早年的Semantic Web还是现在的Linked Data，都缺少来自Web企业的兴趣，缺少主流程序员的参与（虽然有少数例外），造成的后果是工具链缺失，使这些技术还有的一点价值在实际部署中也困难重重。诸如Protege、Pellet和RacePro这些工具，在我看来都是普通程序员和数据处理人员很难接受的、难用的东西。

但是这并不代表语义网整个方向就没有前途了。我也陆续写过一些文章鼓吹所谓的”Lean Semantic Web”的方法，就是从现有的技术出发，从现有的工具链逐步演进，从Web用户的实际需要出发，来发展结构化数据在Web上的应用，来促进数据的流通。我们在实践中综合运用图数据库，全文检索引擎，各种知识提取的方法，基于JSON的数据建模，对程序员友好的规则语言等等。我们认为不应该拘泥于RDF或者任何一种格式，也不应该拘泥于是不是用URI来命名数据。数据从低质量（如杂乱的文本）到高质量（如有良好schema设计和consistent实例的知识图谱）需要付出大量的成本，在部署中不应该过早的引入带来高成本的方法。应该综合运用多种方法，立足于现有的技术，充分利用好现有的工具链，做到逐步投入成本，逐步获得更高的收益。

某些工具在这里提到。不过这个列表是两年前写的了，很多需要补充和更正，仅供参考

https://github.com/baojie/leansemanticweb

## 第二部分

（2014-12-24）

关联数据和RDF是一种数据交换语言。它并不适合作为数据建模语言，也不适合作为数据存储语言。例如Google的Freebase，有RDF的版本，但是它内部的存储和表示都不是RDF。这是一种工业界的一般的最佳实践。标准化的作用在于在各家的内部系统之上，当有数据交换的需要的时候，加一层皮肤。这层皮肤是很浅的。象Oracle等，都有这样的皮肤。

在实际的工程中，通过一种共同的的语言(lingua franca)来解决数据异质性问题，这一直是知识界的理想，奋斗了有三十年了吧。但是Prolog, Lisp, CL, Clips, KIF, KQML, DAML, OIL，直到最近十年的OWL和RDF，都没有一个成功的。我想，这就不是大家努力不够的原因了，而是这个思路本身有问题。

正如在人类社会，人们从讲各种地区性的语言到有共同交流语言，比如汉语和英语，这种lingua franca很少是设计出来的，而是发展出来的。这个发展，总是基于在生活中最普遍使用的语言，在融合各种外来要素，和内部的生产交流要求后，不断的发生微小的演进。偶尔也会发生跃迁，但是这是非常罕见的，在大尺度上几乎不可能发生。

英语里有句话，A language is a dialect with an army and navy，就是说这种共同的的语言其实是一个博弈的结果，它往往是依托于一种势力。这种势力使大家有动力来使用lingua franca。所以共同的的语言的发展中，各种政治的经济的作用要大大优先于语言本身的设计。这不是几个学者设计一个优美的语言就能做到的。如果设计一种语言来统一10种语言，那结果肯定是有了11种需要再次统一的语言。

我想在计算机语言上也会有类似的规律。语义网的精髓在于促进数据的流动，但是数据的流动本身都是权力和利益的转移。语言的产生和发展，要顺从利益，也是各种权力斗争的折衷。在这件事上，W3C有引导的作用，但更主要的推动力来自Google, Apple, Yahooo, Microsoft, Faceebook等巨头，来自成百的创新企业，今后也必然更多的来自中国的企业。新的语言，必然是在他们已经使用的技术上，逐步发展起来。

企业不可能脱离现有的技术栈，一下子就跃迁到一个全新的技术栈去。所以新的知识的交流语言，也必然是从现有的某种广泛使用的技术上发展和演化——JSON可能是其中一种，或许还有YAML。JSON-LD是一个好的方向，虽然还有很多不足。作为个人的偏好，我很看好Python的一个子集来作为知识表现语言（正如JSON和Javascript的关系）。
多种方法的综合使用也是必然的，因为知识表现本身只会是解决方案的一小部分。机器学习、人机交互、数据库，这些都是语义网技术的交叉学科。

【总结】不必拘泥于某一种格式或者方法，只要是结构化数据，或者任何一种有利于提高数据质量的方法，都是语义网的方法。

 ## 第三部分

（2014-12-30 ）

用URI来标定资源这个成本非常高。这个第一步就迈不出去。现实的工程中大家也不这么做。大部分的数据标定还是字符串，或者其他的literals。在字符串的基础上，用数据库查询或者IR的方法，高级一点用NLP的方法，来追求一个近似的标定。这么做才能降低成本。

NLP准确程度和效率开始可能也是不高的，因为目前的知识提取技术离实用还很远，通常都需要bootstrap，就是提取一点，再把这个知识用在系统里做下一轮学习。NELL或者ReVerb都可以参考。但是open-domain的实际效果还是太低，能工程化的通常还是特定领域问题，需要人的知识。

然后规范词汇这个在实践中也非常难以实施，因为这个涉及到很复杂的权力让度，涉及到复杂多变的业务需求和僵化的规范词汇之间的矛盾。上升到ontology wars，就是两个词汇集其实代表了不同的世界观，一个组织的世界观强加给另一个，这个谈何容易。

中文的本体或者英文本体不多，是因为现实中没有这个需要。而且现有的本体，质量太差。比如DBPedia，真正想用到工程里还至少要花几个人年来清理。Freebase也好不到哪里去，所以Google最近把它移交给Wikidata了。

本体是一个非常高的要求，它需要很高的先期投入。除了少数专业领域，比如金融，医学，国防，一般的Web应用承受不起这样的先期投入。一般的合理做法都是从数据库或者IR系统开始，先让系统跑起来，然后看那里面到底有没有什么地方需要把字符串变成URI的？这需要其实非常低，我估计不到10%，也许只有1%。工程师就可以根据实际的使用情况，比如各种日志，来决定哪些地方需要提高数据质量。然后逐步的投入成本，逐步的提高用户体验。反之，如果尽早对每个资源给予固定的唯一的URI，那很有可能这种投资是无效的，是用户不关心的甚至讨厌的（因为他的世界观和你的不一样）。

然后URI之间可能需要发生关系。至于这个关系是不是要映射到规范词汇或者现有的本体上，要看系统和其他系统之间交换的需要。再回顾这个概念，RDF/OWL等的作用是数据交换语言，不是数据存储或者建模语言。当两个组织之间需要交换数据的时候，才有确定一个词汇表或者schema的需要。这个过程也是需要成本的，包括经济的，也有各种政治的成本。所以用一个LOD值词汇，应该服从实际的业务需要，没有真实的，被验证的需求就搞映射，会不必要的提高成本。

总之，把数据从原始的形式，比如日志，数据库或者文本，变成更高质量的形式，比如URI，关系，知识图谱直至本体，不是一个黑白过程，而是一个一步步的，按实际工程需要，按可以承受的成本，按团队实际的经济、政治的能力，来操作的过程。老话说，过早优化是万恶之源，在知识工程也是这样。

***

