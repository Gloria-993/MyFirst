# Boosting和香农信道编码定理

2014.10.19  作者：鲍捷

***

今天看了：周志华教授特邀报告PPT: BOOSTING 25年　<http://vdisk.weibo.com/s/FcILTUAi9m111>

有个想法（发在[微博](http://www.weibo.com/1932835417/BsguBtNNU?mod=weibotime)上）：

我觉得Boosting和香农信道编码定理很像，都是讲一个很烂的(信道/分类器)如何通过冗余来得到精确的效果．读书少，想必有人已经做过比较和证明了。（有噪信道编码定理的证明是统计上最杰出的证明之一）

这里展开再说几句思路。

从信息论的角度，一个分类器的训练过程本质上就是一个信道，它的噪声就是错误．把很多个有错误的分类器组合起来得到一个错误率任意低的分类器，这本质上就和反复使用一个有噪信道，得到一个误码率任意低的信道，可能是一个原理．

在信息论中，反复使用一个信道，依赖于某种带冗余的编码．在香农的经典证明中，他使用了”渐进均分原理”[asymptotic equipartition property](http://en.wikipedia.org/wiki/Asymptotic_equipartition_property) (AEP)．这个原理说，如果我们把话说一百遍啊一万遍啊无数遍，虽然有很多很多可能的组合，其中只有一部分组合是（概率上）值得研究的，其他的完全可以当作路人甲路人乙忽略掉．这些需要研究的组合，就是所谓的榜样集合typical sets，它们出现的概率是一样的．这样就可以把一个任意概率分布的随机过程问题，转化为一个均衡分布的问题来研究，方便大大的．

把一个信道用很多很多次，就可以把一些细节忽略掉，比如单独一个符号的传输错误问题．我们要研究的，是一些符号的ensemble（这个词就慢慢和机器学习联系起来了）能不能被正确传播的概率．用上AEP后，奇妙的事情发生了，我们根本不需要关心具体的编码算法是什么，可以把编码的问题转化为一个几何问题来研究，就是在一个很大的空间里（信道使用的次数越多，这个空间越大），有很多小球，每个对应于一个榜样集合typical set，它之所以是个球是因为有噪声，没有噪声就是个点了，球代表了所有和这个榜样看起来无法区别的错误．一个信道噪声越大，这个榜样集对应的小球半径就越大．

香农证明了，在一定传输速率之下（就是信道容量），如果我们把空间搞得很大很大，那不管那些小球有多大，我们总是有办法把它们之间的距离搞得大大的，这样就能实现任意精确的通信．

现在问题来了：把ensemble（也就是小球们）之间的距离搞得大大的技术，谁家最强？

问题又来了：这怎么看都象是Adaboost的距离理论啊(margin theory)！

在Breiman 1999的证明里， 他证明某个分类器出错的概率上限是：

[![CCL2014_keynote 周志华  1 .pdf](http://baojie.org/blog/wp-content/uploads/2014/10/CCL2014_keynote-%E5%91%A8%E5%BF%97%E5%8D%8E-1-.pdf.png)](http://baojie.org/blog/wp-content/uploads/2014/10/CCL2014_keynote-%E5%91%A8%E5%BF%97%E5%8D%8E-1-.pdf.png)

 

如果我们忽略这个式子里对每个分类器都一样的一些常数，关键的项就是 -RlnR．这个式子非常之象熵．我疑心R这个量和分类器的＂信道容量＂有关．对于一个分类器，如果我们把它的”传输速率”定义为对单位样本输入，获得的正确分类的样本的比例，那它的＂信道容量＂就是在单位样本输入下，我们用某种boosting能获得的正确分类的样本的比例的上限．我猜想对任何一种弱分类器，这个上限是存在的，姑且称为＂分类器容量＂。

这个上限的意义是，在boosting时，是不是用任意大的ensemble就能得到任意好的结果呢？可能不是．在＂分类器容量＂以下，总是能找到一个boosting的办法，在这之上，再怎么努力都不行．也就是说存在一些样本（前提是不可穷举的），不管怎么组合无穷多的分类器，它就依然是错的。

如果我们知道了这个＂分类器容量＂，那对决定用多少个分类器来组合，或许也有好处，也许就不用浪费用过多的分类器。

再胡说一句，如果真有所谓的＂分类器容量＂，那以[The Unreasonable Effectiveness of Data](http://www.datascienceassn.org/sites/default/files/Unreasonable%20Effectiveness%20of%20Data.pdf) 为代表的大数据宗教就不能成立，并不是说数据越多，分类器的效果就会一定越好．存在一个错误的下限，不能做的更好不是数据不够多，而是分类器本身的容量决定的。

以上都是些民科的想法．我不是机器学习专业出身，上面说的可能都是＂香蕉很大香蕉皮更大＂之类的＂发现＂，只是觉得有趣，姑且写下来。

***

